---
title: "Assignment2"
author: "Kyle Bambling"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
```

## Question 1

Write a function that calculates the density function of a Uniform continuous 
variable on the interval $\left(a,b\right)$. The function is defined as 
$$f\left(x\right)=\begin{cases}
\frac{1}{b-a} & \;\;\;\textrm{if }a\le x\le b\\
0 & \;\;\;\textrm{otherwise}
\end{cases}$$
which looks like this
    
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=2}
library(ggplot2) 
library(dplyr) 
data <- data.frame(x = seq(-.25,1.25, length=301)) 
data <- data %>% mutate( y = dunif(x,0,1) ) 
ggplot(data, aes(x=x, y=y)) +
  geom_step() +   # geom_step is similar to geom_line() but for step functions
  scale_x_continuous( breaks=c(0,1), labels=c('a','b') ) +
  scale_y_continuous( breaks=c(0,1), labels=c(0,expression(1 / (b-a) )) ) +
  labs(y='density') + theme_bw()
```

We want to write a function `duniform(x, a, b)` that takes an arbitrary 
value of `x` and parameters a and b and return the appropriate height of 
the density function. For various values of `x`, `a`, and `b`, demonstrate 
that your function returns the correct density value. 
a)  Write your function without regard for it working with vectors of data. 
    Demonstrate that it works by calling the function with a three times, 
    once where $x< a$, once where $a < x < b$, and finally once where $b < x$.  
    
```{r 1-a}
duniform <- function(x, a, b)
{
  answer <- ifelse(x>=a & x<=b, 1/(b-a), 0)
  return(answer)
}

duniform(0,1,3)
duniform(2,1,3)
duniform(4,1,3)
```

b)  Next we force our function to work correctly for a vector of `x` values. 
    Modify your function in part (a) so that the core logic is inside a `for` 
    statement and the loop moves through each element of `x` in succession. 
    Your function should look something like this:
    
```{r, eval=FALSE}
duniform <- function(x, a, b){
  output <- NULL
  for( i in 1:length(x) ){  # Set the for loop to look at each element of x
    if( (x[i] >= a & x[i] <= b) & a!=b ){  # What should this logical expression be?
      output[i] <- 1/(b-a)
    }
    else{
      output[i] <- 0
    }
  }
  return(output)
}
```

Verify that your function works correctly by running the following code:

```{r, eval=FALSE, fig.height=3}
data.frame( x=seq(-1, 12, by=.001) ) %>%
  mutate( y = duniform(x, 4, 8) ) %>%
  ggplot( aes(x=x, y=y) ) +
  geom_step()
```

c)  Install the R package `microbenchmark`. We will use this to discover the 
    average duration your function takes.
    
```{r, eval=FALSE}
microbenchmark::microbenchmark( duniform( seq(-4,12,by=.0001), 4, 8), times=100)
```

This will call the input R expression 100 times and report summary 
statistics on how long it took for the code to run. In particular, look 
at the median time for evaluation.  
d)  Instead of using a `for` loop, it might have been easier to use an 
    `ifelse()` command. Rewrite your function to avoid the `for` loop and just 
    use an `ifelse()` command. Verify that your function works correctly by 
    producing a plot, and also run the `microbenchmark()`. Which version of 
    your function was easier to write? Which ran faster?
    
```{r 1-d}
duniformshort <- function(x, a, b)
{
  answer <- ifelse(x>=a & x<=b, 1/(b-a), 0)
  return(answer)
}
microbenchmark::microbenchmark( duniform( seq(-4,12,by=.0001), 4, 8), times=100)
microbenchmark::microbenchmark( duniformshort( seq(-4,12,by=.0001), 4, 8), times=100)
```

## Question 2

I very often want to provide default values to a parameter that I pass to a 
    function. For example, it is so common for me to use the `pnorm()` and `qnorm()` 
    functions on the standard normal, that R will automatically use `mean=0` and 
    `sd=1` parameters unless you tell R otherwise. To get that behavior, we just 
    set the default parameter values in the definition. When the function is 
    called, the user specified value is used, but if none is specified, the defaults 
    are used. Look at the help page for the functions `dunif()`, and notice that 
    there are a number of default parameters. For your `duniform()` function 
    provide default values of `0` and `1` for `a` and `b`. Demonstrate that your 
    function is appropriately using the given default values. 
    
```{r 2}
duniformshort <- function(x, a=0, b=1)
{
  answer <- ifelse(x>=a & x<=b, 1/(b-a), 0)
  return(answer)
}
duniformshort(seq(-1,3, .5))
```

## Question 3

A common data processing step is to *standardize* numeric variables by subtracting 
the mean and dividing by the standard deviation. Mathematically, the standardized
value is defined as
$$z = \frac{x-\bar{x}}{s}$$
where $\bar{x}$ is the mean and $s$ is the standard deviation.
Create a function that takes an input vector of numerical values and produces
an output vector of the standardized 
values. We will then apply this function to each numeric column in a data frame 
using the `dplyr::across()` or the `dplyr::mutate_if()` commands. 
*This is often done in model algorithms*
*that rely on numerical optimization methods to find a solution. By keeping the*
*scales of different predictor covariates the same, the numerical optimization*
*routines generally work better.*
```{r 3}
standardize <- function(x){
  mean.x <- mean(x)
  sd.x <- sd(x)
  z_score <- (x-mean.x)/sd.x
}

data( 'iris' )
# Graph the pre-transformed data.
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point() +
  labs(title='Pre-Transformation')

# Standardize all of the numeric columns
# across() selects columns and applies a function to them
# there column select requires a dplyr column select command such
# as starts_with(), contains(), or where().  The where() command
# allows us to use some logical function on the column to decide
# if the function should be applied or not.
iris.z <- iris %>% mutate( across(where(is.numeric), standardize) )

# Graph the post-transformed data.
ggplot(iris.z, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point() +
  labs(title='Post-Transformation')
```

## Question 4

In this example, we'll write a function that will output a  vector of the 
    first $n$ terms in the child's game *Fizz Buzz*. The goal is to count as high 
    as you can, but for any number evenly divisible by 3, substitute "Fizz" and 
    any number evenly divisible by 5, substitute "Buzz", and if it is divisible 
    by both, substitute "Fizz Buzz". So the sequence will look like 
    1, 2, Fizz, 4, Buzz, Fizz, 7, 8, Fizz, ... 
    *Hint: The `paste()` function will squish strings together, the remainder*
    *operator is `%%` where it is used as `9 %% 3 = 0`. This problem was inspired*
    *by a wonderful YouTube [video](https://www.youtube.com/watch?v=QPZ0pIK_wsc)*
    *that describes how to write an appropriate loop to do this in JavaScript,*
    *but it should be easy enough to interpret what to do in R. I encourage you*
    *to try to write your function first before watching the video.*
    
```{r 4}
fizz_buzz <- function(n)
{
  n <- 1:16
  return( ifelse( !(n %% 3), ifelse( !(n %% 5), 'Fizz Buzz', 'Fizz' ), ifelse( !(n %% 5), 'Buzz', n ) ) )
}

fizz_buzz(16)
```

## Question 5

The `dplyr::fill()` function takes a table column that has missing values and 
fills them with the most recent non-missing value. For this problem, we will 
create our own function to do the same.

```{r}
#' Fill in missing values in a vector with the previous value.
#' 
#' @parm x An input vector with missing values
#' @result The input vector with NA values filled in.
myFill <- function(x){
  prev_value = NA
  for( i in 1:length(x) )
  {
    ifelse( !is.na(x[i]), prev_value <- x[i], x[i] <- prev_value)
  }
  return(x)
}
```

The following function call should produce the following output

```{r}
test.vector <- c('A',NA,NA, 'B','C', NA,NA,NA)
myFill(test.vector)
```

## Question 6

A common statistical requirement is to create bootstrap confidence intervals 
for a model statistic. This is done by repeatedly re-sampling with replacement 
from our original sample data, running the analysis for each re-sample, and 
then saving the statistic of interest. Below is a function `boot.lm` that 
bootstraps the linear model using case re-sampling.

```{r}
#' Calculate bootstrap CI for an lm object
#' 
#' @param model
#' @param N
boot.lm <- function(model, N=1000){
  data    <- model$model  # Extract the original data
  formula <- model$terms  # and model formula used

  # Start the output data frame with the full sample statistic
  output <- broom::tidy(model) %>% 
    select(term, estimate) %>% 
    pivot_wider(names_from=term, values_from=estimate)

  for( i in 1:N ){
    data.boot <- data %>% sample_frac( replace=TRUE )
    model.boot <- lm( formula, data=data.boot)
    coefs <- broom::tidy(model.boot) %>% 
      select(term, estimate) %>% 
      pivot_wider(names_from=term, values_from=estimate)
    output <- output %>% rbind( coefs )
  }  
  
  return(output)
}

# Run the function on a model
m <- lm( Volume ~ Girth, data=trees )
boot.dist <- boot.lm(m)

# If boot.lm() works, then the following produces a nice graph
boot.dist %>% gather('term','estimate') %>% 
  ggplot( aes(x=estimate) ) + 
  geom_histogram() + 
  facet_grid(.~term, scales='free')
```

